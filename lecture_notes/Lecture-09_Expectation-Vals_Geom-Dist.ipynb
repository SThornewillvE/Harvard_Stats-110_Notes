{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from helper_functions import nCr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Definition\n",
    "\n",
    "When talking about averages there are often different ways to compute it:\n",
    "\n",
    "1. Mean\n",
    "2. Median\n",
    "3. Mode\n",
    "4. Weighted Averages\n",
    "5. Expectation values\n",
    "6. Etc.\n",
    "\n",
    "For us, the average is defined in the discrete case as:\n",
    "\n",
    "$$E(x) = \\frac{1}{n}\\sum_{i=0}^{n}x_i$$\n",
    "\n",
    "We can also think about it like this,\n",
    "\n",
    "$$E(x) = \\sum_{i=0}^{n}xP(X = x)$$\n",
    "\n",
    "The first way this is done is the way how everyone does it, but the second one takes a little more thought. Let's create an array that we can take the average of and see how we can implement these two algorithms as functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 2, 2, 3, 3, 3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 1, 1, 1, 2, 2, 3, 3, 3, 4])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    array_sum = 0\n",
    "    \n",
    "    for i in a:\n",
    "        array_sum+=i\n",
    "    \n",
    "    return array_sum/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_mean(a):\n",
    "    \n",
    "    unique_vals = np.unique(a)\n",
    "    \n",
    "    mean_val = 0\n",
    "    \n",
    "    for i in unique_vals:\n",
    "        prob_i = len(a[a==i])/len(a)\n",
    "        \n",
    "        mean_val+=i*prob_i\n",
    "    \n",
    "    return mean_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our functions, let's try and out and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.1, 2.1, 2.1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(), mean(a), probability_mean(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function might seem a little weird, but it's simply because we take the weighted average for each unique value. The weight depends on the frequency with which it appears in the list.\n",
    "\n",
    "We can now apply this definition to various distributions to see what they expectation values ought to be.\n",
    "\n",
    "## Part 2. Expectation of a bernouli trial\n",
    "\n",
    "Let $X \\sim Bernouli(p)$. We can use our definition of the average above to get the following:\n",
    "\n",
    "$$E(X) = 1P(X=1) + 0P(X=0)$$\n",
    "\n",
    "$$\\therefore E(X) = p$$\n",
    "\n",
    "This is really, really ovvious but it must be said because it's known as `fundamental bridge`.\n",
    "\n",
    "But you would also expect this because if you were to draw from a bernouli distribution infinite times then the probability of success would simply be `p`.\n",
    "\n",
    "## Part 3. Expectation of binomial distribution\n",
    "\n",
    "Now let $X \\sim Binomial(n, p)$.\n",
    "\n",
    "$$E(x) = \\sum_{k=0}^{n}k {n \\choose k} p^k (1-p)^{n-k}$$\n",
    "\n",
    "Note that $k {n \\choose k} = n {n-1 \\choose k-1}$. Why? Because this is the story \"choosing your committee and then choosing a president is the same as choosing a president and then choosing a committee from the remaining people\".\n",
    "\n",
    "$$E(x) = \\sum_{k=0}^{n} n {n-1 \\choose k-1} p^k (1-p)^{n-k}$$\n",
    "\n",
    "We can pull out n and p and adjust the index to get the expectation value equal to `np` by the binomial theorem.\n",
    "\n",
    "$$E(x) = np\\sum_{j=0}^{n-1} {n-1 \\choose j} p^j (1-p)^{n-j-1} = np$$ \n",
    "\n",
    "Where `j = k-1`.\n",
    "\n",
    "Note that this makes sense because of linearity, if a binom distribution is just a series of bernouli trials with an expectation value of `p`, then the expectation of a bernouli trial is equal to how many times you've conducted this trial. \n",
    "\n",
    "See lecture 10 for a proof on linearity.\n",
    "\n",
    "### part 3.5. The Binomial Theorem\n",
    "\n",
    "The binomial theorem states that any sum of two variables to the nth power is equal to:\n",
    "\n",
    "$$(a+b)^n = \\sum_{k = 0}^{n} {n \\choose k} a^k b^{n-k}$$\n",
    "\n",
    "However, what if `a = 1 - b` i.e. the sum of a and b are 1? \n",
    "\n",
    "Well, then \n",
    "\n",
    "$$(a + b)^n = (p + (1-p))^n = p-p+1$$\n",
    "\n",
    "$$\\therefore \\sum_{k = 0}^{n} {n \\choose k} p^k (1-p)^{n-k} = 1$$\n",
    "\n",
    "Because the inside of the completely factorised form will always be equal to 1 and 1 to the nth power will always be 1.\n",
    "\n",
    "We can even show this computationally..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vars\n",
    "n = 10\n",
    "p = 0.5\n",
    "\n",
    "tot = sum([nCr(n, k) * p**(k) * (1-p)**(n-k) for k in range(0, n+1)])\n",
    "\n",
    "tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what values you choose for n and p, the number you get will always be one.\n",
    "\n",
    "This also kind of makes sense because the equation follows this story:\n",
    "\n",
    "\"The sum of the probability of all outcomes of flipping a coin n times should equal to one\". Which makes sense because it is a PMF! \n",
    "\n",
    "## Part 4. Geometric distribution.\n",
    "\n",
    "The geometric distribution is also related to the bernouli trial with the story, \"how many failures until the first success\"?\n",
    "\n",
    "PMF:\n",
    "\n",
    "$$P(X = k) = q^kp = (1-p)^kp$$\n",
    "\n",
    "i.e. \"The probability of k trials being required to get the fist success will be the probability of getting all failures before and the last draw being a success.\" Makes sense.\n",
    "\n",
    "This means that the expectation value is:\n",
    "\n",
    "$$E(X) = \\sum_{k=0}^{\\infty}kq^kp = p\\sum_{k=0}^{\\infty}kq^k$$\n",
    "\n",
    "If you differentiate $\\sum_{k=0}^{\\infty}q^k = \\frac{1}{1-q}$ and mulitply by `q`, then you get $\\sum_{k=0}^{\\infty}kq^{k} = \\frac{q}{(1-q)^2}$\n",
    "\n",
    "We can insert this into the above equation to get\n",
    "\n",
    "$$E(X) = \\sum_{k=0}^{\\infty}kq^kp = \\frac{pq}{p^2} = \\frac{q}{p}$$\n",
    "\n",
    "i.e. \"The expectation is the rate of the rate of failures to successes.\" Note that this is always the same as the rate of successes to failures, this is what we might choose if we did the opposite, the number of successes until the first failure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geom(p, k):\n",
    "    q = (1-p)\n",
    "    return q**k * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
