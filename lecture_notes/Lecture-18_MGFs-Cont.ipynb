{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 18: Moment Generating Functions Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are moments called moments? It comes from physics \"moment of innertia\". Kind of weird, but that's how it is.\n",
    "\n",
    "## Point 1. Exponential MGF\n",
    "\n",
    "Once again, when figuring out the MGF of the exponential, we'll try to find it for when $\\lambda = 1$ and then generalise from there.\n",
    "\n",
    "Recall that $M(t) = E(e^{tx})$\n",
    "\n",
    "If $X \\sim Exp(1)$ Then the PDF is $f(x) = \\lambda e^{-\\lambda x} = e^{-x}$\n",
    "\n",
    "Thus the moment generating function is $M(t) = E(e^{tx})$, through LOTUS we get $M(t) = \\int_0^\\infty e^{tx}e^{-x} dx$\n",
    "\n",
    "We can combine the exponentials and factor it as such $M(t) = \\int_0^\\infty e^{tx-x} dx = \\int_0^\\infty e^{-x(1-t)} dx$\n",
    "\n",
    "$$\\therefore M(t) = \\frac{1}{1-t}, t<1$$\n",
    "\n",
    "To get the expectation value of any X^n, we just need to take the nth derivative of the MGF and evaluate it at 0. \n",
    "\n",
    "We can also recognise that this is a geometric series and so $\\frac{1}{1-t} = \\sum_0^\\infty t^n = \\sum_0^\\infty \\frac{n! t^n}{n!}$. Therefore the nth moment is $n!$.\n",
    "\n",
    "$$\\therefore E(X^n) = n!$$\n",
    "\n",
    "### Point 1.2 Generalising to all exponentials\n",
    "\n",
    "So now what about the general case?\n",
    "\n",
    "let $Y \\sim Exp(\\lambda)$, $X = \\lambda Y \\sim Exp(1)$, then $Y^n = \\frac{X^n}{\\lambda^n}$\n",
    "\n",
    "$$\\therefore E(Y^n) = \\frac{n!}{\\lambda^n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2. Std Norm MGF\n",
    "\n",
    "Let $Z \\sim N(0, 1)$, find all moments of Z.\n",
    "\n",
    "We know that all odd moments of Z are 0 via symmetry.\n",
    "\n",
    "If $M(t) = E(e^{tx})$ then $M(t) = e^{\\frac{t^2}{2}} = \\sum_{n=0}^\\infty \\frac{(\\frac{t^2}{2})^n}{n!} = \\sum_{n=0}^\\infty \\frac{t^{2n}}{2^n n!}$\n",
    "\n",
    "Note that right now the indeces between the numerator and the denominator don't add up. To even this out we multiply and divide by $2n!$\n",
    "\n",
    "$$\\sum_{n=0}^\\infty \\frac{t^{2n}}{2^n n!} = \\sum_{n=0}^\\infty \\frac{(2n)! t^{2n}}{2^n n! (2n)!}$$\n",
    "\n",
    "$$\\therefore E(X^{2n}) = \\frac{(2n)!}{2^n n!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 3. Poisson MGF\n",
    "\n",
    "Let $X \\sim Poisson(\\lambda)$\n",
    "\n",
    "Then $E(e^{tx}) = \\sum_{k=0}^\\infty e^{tk} \\frac{e^{-\\lambda} \\lambda^k}{k!}$\n",
    "\n",
    "$e^{-\\lambda}$ is a constant that can be removed from the sum and note that the remainder is the taylor series that we are looking for. $E(e^{tx}) = e^{-\\lambda} \\sum_{k=0}^\\infty  \\frac{e^{tk} \\lambda^k}{k!}$\n",
    "\n",
    "$$\\therefore E(e^{tx}) = e^{-\\lambda} e^{\\lambda e^t} = e^{\\lambda (e^t-1})$$\n",
    "\n",
    "### Point 3.2 Applying MGF\n",
    "\n",
    "If we have two distributions X and Y that are both Poisson distributed and independent then the distribution of X + Y will be a multiplication of the MGFs, $e^{\\lambda (e^t -1)}e^{\\mu (e^t -1)} = e^{(\\lambda + \\mu) (e^t -1)}$.\n",
    "\n",
    "$$\\therefore X + Y \\sim Poiss(\\lambda + \\mu)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 4. Joint Distributions\n",
    "\n",
    "* Joint CDF\n",
    "\n",
    "$$F(x, y) = P(X\\leq x, Y\\leq y)$$\n",
    "\n",
    "* Joint PMF\n",
    "\n",
    "$$f(x, y) = P(X=x, Y=y)$$\n",
    "\n",
    "* Probability from Joint PMF\n",
    "\n",
    "$$P((X, Y) \\in B) = {\\int\\int}_B f_{X, Y}(x, y)dx dy$$\n",
    "\n",
    "* Marginal\n",
    "\n",
    "$$P(X=x) = \\sum_y P(X=x, Y=y)$$\n",
    "\n",
    "$$f_Y(y) = \\int_{-\\infty}^\\infty f_{X, Y}(x, y) dy$$\n",
    "\n",
    "Note that from Joint distribution we can get the Marginal distribution, but from marginal distributions we might not be able to get the joint back because we don't know how X and Y affect each other.\n",
    "\n",
    "* Independence\n",
    "\n",
    "$$f(x, y) = f_X(x)f_Y(y)$$\n",
    "\n",
    "i.e. If you can take the marginal distributions from joint PMF and recreate the distribution then it is independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point 4.2. Uniform on Squares\n",
    "\n",
    "Let there be a 2 dimensional PDF such that it is uniform in the set $\\{(x, y): x, y \\in [0,1]\\}$.\n",
    "\n",
    "The integral is the area $c = \\frac{1}{\\textrm{area}} = 1$\n",
    "\n",
    "We can get this by multiplying two distributions of $\\textrm{Uniform}(0, 1)$ together, so the marginal distributions are independent.\n",
    "\n",
    "### Point 4.3 Disc\n",
    "\n",
    "Say that we have a uniform distribution where $x^2 + y^2 \\leq 1$\n",
    "\n",
    "The point PDF is $\\frac{1}{\\pi}$ when inside the circle and 0 otherw. Note that X and Y are not independent because if you know X then that decides a little the range that Y can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
