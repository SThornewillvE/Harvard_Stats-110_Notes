{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 22: Transformations and Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1. Transformations\n",
    "\n",
    "Let X be any continuous random variable with PDF $f_x$ and $Y = g(X)$ where g is differentiable and monotonically increasing.\n",
    "\n",
    "What is the PDF of Y?\n",
    "\n",
    "You can achieve this with a transformation\n",
    "\n",
    "$$f_Y(y) = f_X(x) \\frac{dx}{dy}$$\n",
    "\n",
    "Proof:\n",
    "\n",
    "If we have the CDF of Y then $P(Y \\leq y) = P(g(X) \\leq y)$\n",
    "\n",
    "We can invert the g to get $P(X \\leq g^{-1}(y))$, which is simply $P(X \\leq x)$\n",
    "\n",
    "Thus, we can just integrate from there, $\\therefore f_Y(y) = f_X(x) \\frac{dx}{dy}$\n",
    "\n",
    "The key here is that $Y = g(X)$ and $X = g^{-1}(Y)$, i.e. there is a function connecting the two.\n",
    "\n",
    "### Point 1.2 Example of the above\n",
    "\n",
    "suppose that we want to find the PDF for the log-normal distribution.\n",
    "\n",
    "This means we have a $Y = g(Z)$, $g(Z) = e^Z$, $\\therefore g^{-1}(Z) = ln(Z)$\n",
    "\n",
    "Note that $Z \\sim N(0, 1)$\n",
    "\n",
    "This means that $f_Y(y) = \\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-g^{-1}(Z)^2}{2}} \\frac{dz}{dy} = \\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-ln(Z)^2}{2}} \\frac{dz}{dy}$\n",
    "\n",
    "Note that in the above equation we just replace $Z$ for $g^{-1}(Z) = Y$\n",
    "\n",
    "Next, we need to find $\\frac{dz}{dy}$, note that it's easier to find $\\frac{dy}{dz}$ and then invert that.\n",
    "\n",
    "$\\frac{dy}{dz} = e^Z = y$, $\\therefore (\\frac{dy}{dz})^{-1} = \\frac{dz}{dy} = \\frac{1}{y}$\n",
    "\n",
    "$$ \\therefore f_Y(y) = \\frac{1}{\\sqrt{2\\pi}} e^{\\frac{-ln(Z)^2}{2}} \\frac{1}{y}$$\n",
    "\n",
    "### Point 1.3 Jacobian\n",
    "\n",
    "Note that if you do this in ${\\rm I\\!R}^n$ then you have to replace $\\frac{dx}{dy}$ with the jacobian $| \\frac{d\\vec x}{d\\vec y} |$.\n",
    "\n",
    "What is this?\n",
    "\n",
    "It's a matrix where each element is some $\\frac{\\partial X_m}{\\partial Y_n}$ where m is the number of rows and y is the number of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2. Convolutions\n",
    "\n",
    "A convolution is basically a sum of random variables. We have done convolutions back when we did the binomial distribution and we combined a bunch of indicator rvs.\n",
    "\n",
    "i.e. Let T = X + Y and X and Y are independent\n",
    "\n",
    "This means $P(T=t) = \\sum_x P(X = x)P(Y = t-x)$\n",
    "\n",
    "Then, by analogy $f_T(t) = \\int_{-\\infty}^{\\infty} f_X(x)f_Y(t-x)dx$\n",
    "\n",
    "Proof: \n",
    "\n",
    "$F_T(t) = P(T \\leq t)$, we can use the LOTB to get $F_T(t) = \\int_{-\\infty}^{\\infty} P(X+Y \\leq t | X=x)f_X(x) dx$\n",
    "\n",
    "We can then make X be equal to little x and then the condition is dropped $F_T(t) = \\int_{-\\infty}^{\\infty} F_Y(t-x \\leq t)f_X(x) dx$\n",
    "\n",
    "If we then take the derivative of both sides we find that \n",
    "\n",
    "$$f_T(t) = \\int_{-\\infty}^{\\infty} f_X(x)f_Y(t-x)dx$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
