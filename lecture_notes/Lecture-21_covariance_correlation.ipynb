{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 21: Covariance and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1. Covariance \n",
    "\n",
    "Covariance is needed to deal with variances of sums instead of falsely applying linearity.\n",
    "\n",
    "Definition:\n",
    "\n",
    "$$Cov(X, Y) = E((X - E(X)) (Y - E(Y)))$$\n",
    "\n",
    "If X and Y are independent then $Cov(X, Y) = E(X - E(X)) \\cdot E(Y - E(Y))$\n",
    "\n",
    "If X = Y then Cov(X, Y) is just the Variance of either X or Y.\n",
    "\n",
    "Note as well that you can expand the definition for the expectation value above to get $Cov(X, Y) = E(XY) - E(Y)E(X)$\n",
    "\n",
    "Also note that the covariance with any random variable and a constant is 0.\n",
    "\n",
    "Also note, if X and Y are independent then the Covariance of the two is 0. However, the reverse is not necessarily true! Dependent variables can still have a covariance of 0. (See the case where X is a rv, Y = X and Z = X\\*X. Y and Z have a Cov. of 0 but they are perfectly dependent. \n",
    "\n",
    "### Point 1.2. Bilinearity\n",
    "\n",
    "Covariance is bilinear, this means if we hold all variables constant, we can treat the remaining one as being linear:\n",
    "\n",
    "$$Cov(cX, Y) = c Cov(X, Y)$$\n",
    "\n",
    "$$Cov(X, Y+Z) = Cov(X, Y) + Cov(X, Z)$$\n",
    "\n",
    "### Point 1.3 Sum of rvs\n",
    "\n",
    "This means that if X is a random variable then:\n",
    "\n",
    "$$Var(X_1 + X_2 +  ... + X_n) = \\sum_{i=1}^n Var(X_i) + 2\\sum_{i<j} Cov(X_i, X_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2. Correlation\n",
    "\n",
    "Using covariance we can now define correlation, often denoted by $\\rho$.\n",
    "\n",
    "$$Corr(X, Y) = \\frac{Cov(X, Y)}{SD(X) SD(Y)}, SD(X) = \\sqrt{Var(X)}$$\n",
    "\n",
    "The advantage of using correlation is that it is easier to interpret, but Covariance is easier to work with mathematically. \n",
    "\n",
    "Note that Corr has a range between -1 and 1. (There's a proof but I don't care for it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
