{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 19: Joint, Conditional and Marginal Distributions\n",
    "\n",
    "What happens when you work with multiple random variables?\n",
    "\n",
    "## Point 1. Extending conditions from last time\n",
    "\n",
    "* Joint CDF\n",
    "\n",
    "$$F(x, y) = P(X \\leq x, Y \\leq y)$$\n",
    "\n",
    "* Joint PDF\n",
    "\n",
    "$$f(x, y) = \\frac{\\partial^2}{\\partial x \\partial y} F(x, y)$$\n",
    "\n",
    "$$P((X, Y) \\in A) = \\int \\int_A f(x, y) dx dy$$\n",
    "\n",
    "* Marginal PDF\n",
    "\n",
    "Integrate over all values you want to remove\n",
    "\n",
    "* Conditional Probability\n",
    "\n",
    "$$f_{Y|X}(y|x) = \\frac{f_{X, Y}(x, y)}{f_X(x)}$$\n",
    "\n",
    "$$f_{Y|X}(y|x) = \\frac{f_{X|Y}(x|y) f_Y(y)}{F_X(x)}$$\n",
    "\n",
    "### Point 1.2. Example with disc\n",
    "\n",
    "Recall disc example from last time. We can find the marginal distribution of X via $f_X(x) = \\int_{-\\infty}^\\infty f(x, y) dy = \\int_{-\\sqrt{1-x^2}}^{\\sqrt{1-x^2}}\\frac{1}{\\pi}dy$\n",
    "\n",
    "This is equal to $\\frac{2}{\\pi}\\sqrt{1-x^2}, -1 \\leq x \\leq 1$\n",
    "\n",
    "We can use this to find $f_{Y|X}(y|x) = \\frac{\\frac{1}{\\pi}}{\\frac{2}{\\pi}\\sqrt{1-x^2}} = \\frac{1}{2\\sqrt{1-x^2}}$\n",
    "\n",
    "Note that this last result doesnt have y, which means it is uniform within the correct domain. \n",
    "\n",
    "To prove once and for all that X and Y are not independent then you can get both marginal distributions and try and recreate the joint distribution by multiplying them together, which won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2. 2 Dimensional LOTUS\n",
    "\n",
    "If we have two variables then LOTUS is still analogous to how it is with one dimension\n",
    "\n",
    "$$E(g(X, Y)) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x, y)f(x, y) dx dy$$\n",
    "\n",
    "### Point 2.2. Applying 2D LOTUS\n",
    "\n",
    "We can apply 2d LOTUS to show $E(XY) = E(X) E(Y)$ if X and Y are independent\n",
    "\n",
    "This is because $E(XY) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} x y f(x) f(y) dx dy$\n",
    "\n",
    "Note that since the first integral does not depend on Y we can just treat it as constant and only deal with the X variables and vice versa.\n",
    "\n",
    "This means pretty easily that $E(XY) = E(X) E(Y)$, QED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 3 Chicken and Egg Problem\n",
    "\n",
    "Let $N \\sim Poiss(\\lambda)$ number of eggs. Each hatches w/ prob P independently with X being the number that hatch. (Note that $X|N \\sim \\textrm{Binomial}(N, p)$. Also, let y be those that dont hatch such that $X + Y = N$. \n",
    "\n",
    "What is the joint PMF of X & Y?\n",
    "\n",
    "We can think of $P(X=i, Y=j) = \\sum_{n=0}^\\infty P(X=i, Y=j | N=n) P(N=n)$\n",
    "\n",
    "Since $N = i + j$, $P(X=i, Y=j) = \\sum_{n=0}^\\infty P(X=i, Y=j | N=i+j) P(N=i+j)$\n",
    "\n",
    "We know that X and Y must add to N so the Y variable creates no new info, $P(X=i, Y=j) = \\sum_{n=0}^\\infty P(X=i | N=n) P(N=n)$\n",
    "\n",
    "Hence, $P(X=i, Y=j) = \\frac{(i+j)!}{i! j!} p^i q^j \\frac{e^{-\\lambda}\\lambda^{i+j}}{(i+j)!}$\n",
    "\n",
    "This means that X and Y can be seen as the multiplcation of two poisson functions where $X \\sim \\textrm{Poiss}(\\lambda p)$ and $Y \\sim \\textrm{Poiss}(\\lambda q)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
